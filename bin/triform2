#!/usr/bin/env python

import argparse
import pandas as pd
import numpy as np
import pyranges as pr
from pyrle import PyRles
from scipy.stats import norm
import scipy.signal as ss

from triform2.version import __version__

parser = argparse.ArgumentParser(
    description="""Triform2: Improved sensitivity, specificity and control of false discovery rates in ChIP-Seq peak finding.

(Visit github.com/endrebak/triform2 for examples and help.)""")

parser.add_argument('--version',
                    '-v',
                    action='version',
                    version='%(prog)s {}'.format(__version__))

parser.add_argument(
    '--treatment',
    '-t',
    required=True,
    type=str,
    nargs='+',
    help='''Treatment (pull-down) file(s) in bam/bed/bed.gz/bed.bz2 format.
                   ''')

parser.add_argument(
    '--control',
    '-c',
    required=False,
    type=str,
    nargs='+',
    help='''Control (input) file(s) in bam/bed/bed.gz/bed.bz2 format.''')

parser.add_argument(
    '--read-width',
    '-rw',
    required=False,
    default=100,
    type=int,
    help=
    '''[Advanced] Read width w, symmetrically extended to a fixed value. Default: 100 bp.''')

parser.add_argument(
    '--min-enrichment',
    '-mr',
    required=False,
    default=0.375,
    type=float,
    help=
    '''[Advanced] Minimum local enrichment ratio (default 3/8 quantile of the enrichment ratio)''')


parser.add_argument(
    '--min-width',
    '-mw',
    required=False,
    default=10,
    type=int,
    help=
    '''[Advanced] Minimum number of bp (peak width) in peak-like region (default 10 bp).''')

parser.add_argument(
    '--max-p',
    '-mp',
    required=False,
    default=0.1,
    type=float,
    help=
    '''[Advanced] Used to calculate minimum upper-tail z-value (default corresponds to standard normal p = 0.1)''')


parser.add_argument(
    '--flank-distance',
    '-fd',
    required=False,
    default=150,
    type=int,
    help=
    '''[Advanced] Fixed spacing between central and flanking locations (must be > read-width). Default: 150 bp.''')

parser.add_argument(
    '--min-shift',
    '-ms',
    required=False,
    default=10,
    type=int,
    help=
    '''[Advanced] Minimum inter-strand shift (lag) between peak coverage distributions (default 10 bp).''')

args = vars(parser.parse_args())
gr = pr.PyRanges(pd.read_parquet(args["treatment"][0]))


flank_distance = args["flank_distance"]
read_width = args["read_width"]

max_p = args["max_p"]
min_width = args["min_width"]
min_shift = args["min_shift"]
min_enrichment = args["min_enrichment"]

from scipy.stats import norm
min_z = norm.ppf(1 - max_p)


def ccf(x, y, lag_max = 100):

    result = ss.correlate(y - np.mean(y), x - np.mean(x), method='direct') / (np.std(y) * np.std(x) * len(y))
    length = (len(result) - 1) // 2
    lo = length - lag_max
    hi = length + (lag_max + 1)

    return result[lo:hi]


def compute_lag(n, p):
    res = ccf(n, p)
    return res.argmax() - 100

def compute_lags(peaks, cvg):

    lags = []
    # do in parallel?
    for k in peaks.chromosomes:
        df = peaks[k].df
        df = df["Start End".split()]

        p = cvg[k, "+"][df]
        n = cvg[k, "-"][df]

        pv = np.repeat(p.Value, p.Run).values
        pi = np.repeat(p.ID, p.Run).values
        nv = np.repeat(n.Value, n.Run).values
        df = pd.DataFrame({"ID": pi, "PV": pv, "NV": nv})
        lags.extend(df.groupby("ID").apply(lambda df: compute_lag(df.NV, df.PV)).values)

    return np.array(lags)


def extend_df(df, kwargs):

    df = df.copy()
    read_width = kwargs["read_width"]
    width = df.End - df.Start
    gaps = np.array(np.floor((read_width - width) / 2), dtype=np.long)
    df.Start = df.Start - gaps - 1
    df.End = df.Start + width + (2 * gaps)

    return df


# gr = gr.apply(extend_df)
# rles = gr.to_rle(strand=True)




if __name__ == "__main__":

    from collections import defaultdict
    import pyranges as pr
    import pandas as pd
    import numpy as np
    read_width = 100
    flank_distance = 150

    gr = pr.PyRanges(pd.read_parquet("example_data/srf_huds_Gm12878_rep1.pq")).drop_duplicate_positions()
    grb = pr.PyRanges(pd.read_parquet("example_data/backgr_huds_Gm12878_rep1.pq")).drop_duplicate_positions()

    gr = gr.apply(extend_df, read_width = read_width)
    grb = grb.apply(extend_df, read_width = read_width)

    ratios = {"+": len(grb["+"]) / len(gr["+"]),
              "-": len(grb["-"]) / len(gr["-"])}

    nan_and_inf_to_0 = lambda x: np.nan_to_num(x, posinf=0, neginf=0)
    def above_cutoff_to_ranges(r, c, k):
        r = r.copy()
        r.values[r.values < c] = 0
        r.values[r.values >= c] = 1
        r = r.defragment()
        return PyRles({k: r}).to_ranges()


    def make_denominator(r):
        r.values = nan_and_inf_to_0(np.sqrt(r.values))
        return r

    def binarize(v):
        v[v < 0] = 0
        v[v > 0] = 1
        return v

    def pnorm(max_z):

        np.seterr(divide="ignore")
        r = np.log(1 - norm.cdf(max_z))
        np.seterr(divide="warn")

        return r


    def find_peaks(rle, left, right, rle_b, ratio, chromosome, strand):

        eq5_numerator = rle - left

        eq6_numerator = rle - right

        eq4_numerator = eq5_numerator + eq6_numerator

        eq4_denominator = make_denominator(2 * (rle + right + left))

        z1 = (eq4_numerator / eq4_denominator).apply_values(nan_and_inf_to_0)

        eq5_denominator = make_denominator(rle + left)
        z2 = (eq5_numerator / eq5_denominator).apply_values(nan_and_inf_to_0)

        eq6_denominator = make_denominator(rle + right)
        z3 = (eq6_numerator / eq6_denominator).apply_values(nan_and_inf_to_0)

        eq8_numerator = ((ratios[strand] * rle) - rle_b)
        eq8_denominator = make_denominator(ratios[strand] * (rle_b + rle))
        z4 = (eq8_numerator / eq8_denominator).apply_values(nan_and_inf_to_0)

        k = (chromosome, strand)
        peaks1 = above_cutoff_to_ranges(z1, min_z, k)
        peaks2 = above_cutoff_to_ranges(z2, min_z, k)
        peaks3 = above_cutoff_to_ranges(z3, min_z, k)
        peaks4 = above_cutoff_to_ranges(z4, min_z, k)

        peaks1 = peaks1.intersect(peaks4)
        peaks2 = peaks2.intersect(peaks4)
        peaks3 = peaks3.intersect(peaks4)

        peaks1 = peaks1[peaks1.lengths() >= min_width]
        peaks2 = peaks2[peaks2.lengths() >= min_width]
        peaks3 = peaks3[peaks3.lengths() >= min_width]

        peaks1, peaks2, peaks3 = z1[peaks1], z2[peaks2], z3[peaks3]
        peaks1.Type, peaks2.Type, peaks3.Type = 1, 2, 3

        return peaks1, peaks2, peaks3

    all_peaks = defaultdict(dict)
    lag_cvg = dict()
    for chromosome in gr.chromosomes:

        if not (len(gr[chromosome, "+"]) > 0 and len(gr[chromosome, "-"]) > 0):
            print(chromosome + " does not contain data from both strands.")
            continue

        for strand in "+-":
            _gr = gr[chromosome, strand]
            _grb = grb[chromosome, strand]
            rle = _gr.to_rle(strand=True)[chromosome, strand]
            rle_b = _grb.to_rle(strand=True)[chromosome, strand]

            left = rle.shift(-flank_distance)
            right = rle.shift(flank_distance)

            ratio = ratios[strand]
            peaks1, peaks2, peaks3 = find_peaks(rle, left, right, rle_b, ratio, chromosome, strand)

            for peaks in [peaks1, peaks2, peaks3]:
                max_z = lambda df: df.groupby(["Chromosome", "Start", "End", "Strand", "Type"], observed=True, as_index=False).Value.max()
                peaks = peaks.apply(max_z).apply(lambda df: df.astype({"Start": np.int32, "End": np.int32}))
                peaks.NLP = np.around(-(pnorm(peaks.Value)/np.log(10)), 3)
                peaks.Location = np.ceil((peaks.Start + peaks.End)/2).astype(int)
                peaks = peaks.drop("Value")
                cvg = rle[peaks.Location]
                bg_cvg = rle_b[peaks.Location]
                peaks.Enrichment = ((ratio * cvg) + 1) / (bg_cvg + 1)

                cvg_left = left[peaks.Location]
                cvg_right = right[peaks.Location]

                peaks.SURL = cvg_left
                peaks.SURR = cvg_right
                peaks.CVG = cvg

                peak_type = peaks.head(1).Type.iloc[0]
                all_peaks[peak_type][chromosome, strand] = peaks.df

            mult = pr.concat([p.drop() for p in [peaks1, peaks2, peaks3]]).slack(flank_distance).merge().to_rle()[chromosome, strand]
            lag_cvg[chromosome, strand] = rle * mult
            # lag_cvg[chromosome, strand] = subsetted.df

    lag_cvg = PyRles(lag_cvg)
    # print(lag_cvg)
    all_peaks2 = []
    # enrichments = defaultdict(list)

    for peak_type, peak_dict in all_peaks.items():
        # print([df.Start.dtype for df in peak_dict.values()])
        gr = pr.PyRanges(peak_dict)
        # print(gr)
        gr.ID = np.arange(len(gr))
        stranded_grs = {}
        for strand in "+-":
            _gr = gr[strand]
            cutoff = np.percentile(_gr.Enrichment, min_enrichment * 100)
            _gr = _gr.subset(lambda df: df.Enrichment > cutoff)
            stranded_grs[strand] = _gr[["Enrichment", "ID"]]

        j = stranded_grs["+"].join(stranded_grs["-"]).new_position("union").drop(like="Start_|End_|Strand").unstrand()
        j = j.apply(lambda df: df.reindex(df.groupby(["ID"], observed=True, as_index=False).Enrichment.idxmax()))
        j = j.apply(lambda df: df.reindex(df.groupby(["ID_b"], observed=True, as_index=False).Enrichment_b.idxmax()))

        if peak_type == 1:
            j.Start -= flank_distance
            j.End += flank_distance
        elif peak_type == 2:
            j.Start -= flank_distance
        elif peak_type == 3:
            j.End += flank_distance


        j.Lag = compute_lags(j, lag_cvg)
        j = j.subset(lambda df: df.Lag > min_shift)
        p = gr["+"]
        n = gr["-"]
        p = p[p.ID.isin(j.ID)]
        n = n[n["-"].ID.isin(j.ID_b)]
        new_ends = np.minimum(p.End.values, n.End.values) 
        out = pr.PyRanges(chromosomes=p.Chromosome, starts=new_starts, ends=new_ends)
        out.CVG = p.CVG.values + n.CVG.values
        out.SURL = p.SURL.values + n.SURL.values
        out.SURR = p.SURR.values + n.SURR.values
        out.Loc = (np.round((p.Location.values + n.Location.values)/2)).astype(int)
        out.Type = p.Type.values
        all_peaks.append(out)


        # def _zscores(x, y, r=1):

        #     diff = (r * x) - y
        #     zs = diff/np.sqrt(r * (x + y))

        #     return zs
        # if peak_type == 1:
        #     zs = _zscores(_cvg, surl + surr, 2)
        #     max_zs = _zscores(_cvg + surl + surr, 0, 2)
        # elif peak_type == 2:
        #     zs = _zscores(_cvg, surl)
        #     max_zs = _zscores(_cvg + surl, 0)
        # elif peak_type == 3:
        #     zs = _zscores(_cvg, surr)
        #     max_zs = _zscores(_cvg + surr, 0)

        # print(good_plus)
        # print(good_neg)
        # print("-----" * 5)
        # print(j)
        # r = gr.overlap(j)
        # print(r)

    # peaks = pr.concat(all_peaks2)
    # print(peaks)

    # print(peaks.Type.value_counts())

    # p = peaks.drop()
    # p.join(p, strandedness="opposite")


        # print(_gr.msp())
    # true peaks:
        # find peaks that overlap on forward and reverse strand
        # join strandedness reverse then new pos union

        # then add flank_distance to peaks
        # if peak_type == 1:
        #     new_peaks.Start -= flank_distance
        #     new_peaks.End += flank_distance
        # elif peak_type == 2:
        #     new_peaks.Start -= flank_distance
        # elif peak_type == 3:
        #     new_peaks.End += flank_distance

        # then compute their lags with sum_cvg 
        # then their lags

