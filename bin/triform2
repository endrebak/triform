#!/usr/bin/env python

import sys
import argparse
import pandas as pd
import numpy as np
import pyranges as pr
from pyrle import PyRles
from scipy.stats import norm
import scipy.signal as ss

from triform2.version import __version__

parser = argparse.ArgumentParser(
    description="""Triform2: Improved sensitivity, specificity and control of false discovery rates in ChIP-Seq peak finding.

(Visit github.com/endrebak/triform2 for examples and help.)""")

parser.add_argument('--version',
                    '-v',
                    action='version',
                    version='%(prog)s {}'.format(__version__))

parser.add_argument(
    '--treatment',
    '-t',
    required=True,
    type=str,
    nargs='+',
    help='''Treatment (pull-down) file(s) in bam/bed/bed.gz/bed.bz2 format.
                   ''')

parser.add_argument(
    '--control',
    '-c',
    required=False,
    type=str,
    nargs='+',
    help='''Control (input) file(s) in bam/bed/bed.gz/bed.bz2 format.''')


parser.add_argument(
    '--false-discovery-rate',
    '-fdr',
    required=False,
    default=0.05,
    type=float,
    help=
    '''Remove all peaks with an FDR above cutoff. Default 0.05.''')



parser.add_argument(
    '--read-width',
    '-rw',
    required=False,
    default=100,
    type=int,
    help=
    '''[Advanced] Read width w, symmetrically extended to a fixed value. Default: 100 bp.''')

parser.add_argument(
    '--min-enrichment',
    '-mr',
    required=False,
    default=0.375,
    type=float,
    help=
    '''[Advanced] Minimum local enrichment ratio (default 3/8 quantile of the enrichment ratio)''')


parser.add_argument(
    '--min-width',
    '-mw',
    required=False,
    default=10,
    type=int,
    help=
    '''[Advanced] Minimum number of bp (peak width) in peak-like region (default 10 bp).''')

parser.add_argument(
    '--max-p',
    '-mp',
    required=False,
    default=0.1,
    type=float,
    help=
    '''[Advanced] Used to calculate minimum upper-tail z-value (default corresponds to standard normal p = 0.1)''')


parser.add_argument(
    '--flank-distance',
    '-fd',
    required=False,
    default=150,
    type=int,
    help=
    '''[Advanced] Fixed spacing between central and flanking locations (must be > read-width). Default: 150 bp.''')

parser.add_argument(
    '--min-shift',
    '-ms',
    required=False,
    default=10,
    type=int,
    help=
    '''[Advanced] Minimum inter-strand shift (lag) between peak coverage distributions (default 10 bp).''')

args = vars(parser.parse_args())
# gr = pr.PyRanges(pd.read_parquet(args["treatment"][0]))


flank_distance = args["flank_distance"]
read_width = args["read_width"]

fdr = args["false_discovery_rate"]
max_p = args["max_p"]
min_width = args["min_width"]
min_shift = args["min_shift"]
min_enrichment = args["min_enrichment"]

from scipy.stats import norm
min_z = norm.ppf(1 - max_p)


def read_files(files):
    cvg = PyRles()
    lengths = {}

    for f in files:
        print("Reading", f, file=sys.stderr)
        if f.endswith(".bed") or f.endswith(".bed.gz"):
            df = pd.read_table(f, header=None, usecols=[0, 1, 2, 5], dtype={0: "category", 1: np.int32, 2: np.int32, 5: "category"}, nrows=None)
            df.columns = "Chromosome Start End Strand".split()
        elif f.endswith(".pq"):
            df = pd.read_parquet(f)
        elif f.endswith(".bam"):
            df = pr.read_bam(output_df=True)

        gr = pr.PyRanges(df)

        lengths["+"] = len(gr["+"])
        lengths["-"] = len(gr["-"])

        cvg = gr.apply(extend_df, read_width=read_width).to_rle()

    return cvg, lengths


def compute_fdr(df, fdr):

    print("Using an FDR of {}.".format(fdr), file=sys.stderr)

    df = df.sort_values("NLP", ascending=False)

    unique_nlp = df.NLP.drop_duplicates()

    sizes = unique_nlp.apply(lambda x: (df.NLP == x).sum())
    indices = unique_nlp.apply(lambda x: (df.NLP >= x).sum())

    nlrs = []
    for nlp, j in zip(unique_nlp, indices):
        m = sum(df.MAX_NLP >= nlp)
        by = np.log10(1/sum(range(1, m + 1)))
        nls = nlp + np.log10(j/m)
        nlrs.append(max(nls - by, 0))

    nlqs = [max(nlrs[i:]) for i in range(len(nlrs))]

    nlqss = np.repeat(nlqs, sizes)

    qvals = 10 ** -nlqss

    df.insert(df.shape[1], "QVAL", qvals)

    n_total = len(df)
    df = df[df.QVAL <= fdr]

    n_passed = (df.QVAL <= fdr).sum()

    print("{} of {} peaks passed the FDR cutoff.".format(n_passed, n_total), file=sys.stderr)

    return pr.PyRanges(df)


def ccf(x, y, lag_max = 100):

    result = ss.correlate(y - np.mean(y), x - np.mean(x), method='direct') / (np.std(y) * np.std(x) * len(y))
    length = (len(result) - 1) // 2
    lo = length - lag_max
    hi = length + (lag_max + 1)

    return result[lo:hi]


def compute_lag(n, p):
    res = ccf(n, p)
    return res.argmax() - 100

def compute_lags(peaks, cvg):

    lags = []
    for k in peaks.chromosomes:
        df = peaks[k].df
        df = df["Start End".split()]

        p = cvg[k, "+"][df]
        n = cvg[k, "-"][df]

        pv = np.repeat(p.Value, p.Run).values
        pi = np.repeat(p.ID, p.Run).values
        nv = np.repeat(n.Value, n.Run).values

        df = pd.DataFrame({"ID": pi, "PV": pv, "NV": nv})
        lags.extend(df.groupby("ID").apply(lambda df: compute_lag(df.NV, df.PV)).values)

    return np.array(lags)


def extend_df(df, kwargs):
    read_width = kwargs["read_width"]


    width = df.End - df.Start
    gaps = np.array(np.floor((read_width - width) / 2), dtype=np.long)
    df.Start = df.Start - gaps - 1
    df.End = df.Start + width + (2 * gaps)

    return df



if __name__ == "__main__":

    from collections import defaultdict
    import pyranges as pr
    import pandas as pd
    import numpy as np
    read_width = 100
    flank_distance = 150

    from time import time
    start = time()
    chip_rles, chip_length = read_files(args["treatment"])
    end = time()
    print("took", end - start)
    start = time()
    background_rles, background_length = read_files(args["control"])
    end = time()
    print("took", end - start)


    ratios = {"+": background_length["+"] / chip_length["+"],
              "-":background_length["-"] / chip_length["-"]}

    nan_and_inf_to_0 = lambda x: np.nan_to_num(x, posinf=0, neginf=0)
    def above_cutoff_to_ranges(r, c, k):
        r = r.copy()
        r.values[r.values < c] = 0
        r.values[r.values >= c] = 1
        r = r.defragment()
        return PyRles({k: r}).to_ranges()


    def make_denominator(r):
        r.values = nan_and_inf_to_0(np.sqrt(r.values))
        return r

    def binarize(v):
        v[v < 0] = 0
        v[v > 0] = 1
        return v

    def pnorm(max_z):

        r = -np.log(norm.sf(max_z))/np.log(10)

        return r

    def _zscores(x, y, r=1):
        diff = (r * x) - y
        return diff/np.sqrt(r * (x + y))


    def find_peaks(rle, left, right, rle_b, ratio, chromosome, strand):

        eq5_numerator = rle - left

        eq6_numerator = rle - right

        eq4_numerator = eq5_numerator + eq6_numerator

        eq4_denominator = make_denominator(2 * (rle + right + left))

        z1 = (eq4_numerator / eq4_denominator).apply_values(nan_and_inf_to_0)

        eq5_denominator = make_denominator(rle + left)
        z2 = (eq5_numerator / eq5_denominator).apply_values(nan_and_inf_to_0)

        eq6_denominator = make_denominator(rle + right)
        z3 = (eq6_numerator / eq6_denominator).apply_values(nan_and_inf_to_0)

        _tmp = ratios[strand] * rle
        eq8_numerator = (_tmp - rle_b)
        eq8_denominator = make_denominator(ratios[strand] * (rle_b + rle))
        z4 = (eq8_numerator / eq8_denominator).apply_values(nan_and_inf_to_0)

        k = (chromosome, strand)
        peaks1 = above_cutoff_to_ranges(z1, min_z, k)
        peaks2 = above_cutoff_to_ranges(z2, min_z, k)
        peaks3 = above_cutoff_to_ranges(z3, min_z, k)
        peaks4 = above_cutoff_to_ranges(z4, min_z, k)

        peaks1 = peaks1.intersect(peaks4)
        peaks2 = peaks2.intersect(peaks4)
        peaks3 = peaks3.intersect(peaks4)

        peaks1 = peaks1[peaks1.lengths() >= min_width]
        peaks2 = peaks2[peaks2.lengths() >= min_width]
        peaks3 = peaks3[peaks3.lengths() >= min_width]

        peaks1, peaks2, peaks3 = z1[peaks1], z2[peaks2], z3[peaks3]
        peaks1.Type, peaks2.Type, peaks3.Type = 1, 2, 3

        return peaks1, peaks2, peaks3

    all_peaks = defaultdict(dict)
    lag_cvg = dict()
    chromosomes = chip_rles.chromosomes
    for chromosome in chromosomes:

        print(chromosome, file=sys.stderr)

        for strand in "+-":
            rle = chip_rles[chromosome, strand]

            rle_b = background_rles[chromosome, strand]
            s = pd.Series(rle_b.values)
            left = rle.shift(-flank_distance)
            right = rle.shift(flank_distance)

            ratio = ratios[strand]
            peaks1, peaks2, peaks3 = find_peaks(rle, left, right, rle_b, ratio, chromosome, strand)

            for peaks in [peaks1, peaks2, peaks3]:
                if not len(peaks):
                    continue

                max_z = lambda df: df.groupby(["Chromosome", "Start", "End", "Strand", "Type"], observed=True, as_index=False).Value.max()
                peaks = peaks.apply(max_z).apply(lambda df: df.astype({"Start": np.int32, "End": np.int32}))

                peaks.NLP = np.around((pnorm(peaks.Value)), 3)
                peaks.Location = np.ceil((peaks.Start + peaks.End)/2).astype(int)
                peaks = peaks.drop("Value")
                cvg = rle[peaks.Location]
                bg_cvg = rle_b[peaks.Location]
                peaks.Enrichment = ((ratio * cvg) + 1) / (bg_cvg + 1)

                cvg_left = left[peaks.Location]
                cvg_right = right[peaks.Location]

                peaks.SURL = cvg_left
                peaks.SURR = cvg_right
                peaks.CVG = cvg

                peak_type = peaks.head(1).Type.iloc[0]
                all_peaks[peak_type][chromosome, strand] = peaks.df

            mult = pr.concat([p["Chromosome Start End".split()] for p in [peaks1, peaks2, peaks3]]).slack(flank_distance).merge().to_rle()[chromosome, strand]
            lag_cvg[chromosome, strand] = rle * mult

    lag_cvg = PyRles(lag_cvg)
    lag_cvg = lag_cvg.make_strands_same_length()
    peaks = defaultdict(lambda: pr.PyRanges())

    for peak_type, peak_dict in all_peaks.items():
        gr = pr.PyRanges(peak_dict)
        gr.ID = np.arange(len(gr))
        stranded_grs = {}
        for strand in "+-":
            _gr = gr[strand]
            cutoff = np.percentile(_gr.Enrichment, min_enrichment * 100)
            _gr = _gr.subset(lambda df: df.Enrichment > cutoff)
            stranded_grs[strand] = _gr[["Enrichment", "ID"]]

        j = stranded_grs["+"].join(stranded_grs["-"]).new_position("union").drop(like="Start_|End_|Strand").unstrand()
        j = j.apply(lambda df: df.reindex(df.groupby(["ID"], observed=True, as_index=False).Enrichment.idxmax()))
        j = j.apply(lambda df: df.reindex(df.groupby(["ID_b"], observed=True, as_index=False).Enrichment_b.idxmax()))

        if peak_type == 1:
            j.Start -= flank_distance
            j.End += flank_distance
        elif peak_type == 2:
            j.Start -= flank_distance
        elif peak_type == 3:
            j.End += flank_distance

        # j.to_csv(str(peak_type) + "_peak_type.tsv", sep="\t")
        j.Lag = compute_lags(j, lag_cvg)
        j = j.subset(lambda df: df.Lag > min_shift)
        p = gr["+"]
        n = gr["-"]
        p = p[p.ID.isin(j.ID)]
        n = n[n["-"].ID.isin(j.ID_b)]
        new_starts = np.maximum(p.Start.values, n.Start.values) 
        new_ends = np.minimum(p.End.values, n.End.values) 
        out = pr.PyRanges(chromosomes=p.Chromosome, starts=new_starts, ends=new_ends)
        cvg = p.CVG.values + n.CVG.values
        surl = p.SURL.values + n.SURL.values
        surr = p.SURR.values + n.SURR.values

        if peak_type == 1:
            zs = _zscores(cvg, surl + surr, 2)
            max_zs = _zscores(cvg + surl + surr, 0, 2)
        elif peak_type == 2:
            zs = _zscores(cvg, surl)
            max_zs = _zscores(cvg + surl, 0)
        elif peak_type == 3:
            zs = _zscores(cvg, surr)
            max_zs = _zscores(cvg + surr, 0)

        peak_nlp = pnorm(zs)
        max_nlp = pnorm(max_zs)

        out.CVG = cvg
        out.SURL = surl
        out.SURR = surr
        out.Loc = (np.round((p.Location.values + n.Location.values)/2)).astype(int)
        out.Type = p.Type.values

        out.NLP = peak_nlp
        out.MAX_NLP = max_nlp

        peaks[peak_type] = out

    # remove overlapping
    peaks[2] = peaks[2].overlap(peaks[1], invert=True)
    peaks[3] = peaks[3].overlap(peaks[1], invert=True)
    peaks[3] = peaks[3].overlap(peaks[2], invert=True)

    result = pr.concat(list(peaks.values()))

    print(compute_fdr(result.df, fdr))
